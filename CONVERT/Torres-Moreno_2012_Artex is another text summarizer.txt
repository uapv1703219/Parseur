Artex is AnotheR TEXt summarizer

Juan-Manuel Torres-Moreno1,2

1 Laboratoire Informatique d’Avignon,

BP 91228 84911, Avignon, Cedex 09, France

juan-manuel.torres@univ-avignon.fr

2 École Polytechnique de Montréal,

CP. 6128 succursale Centre-ville, Montréal, Québec, Canada

2
1
0
2

 
t
c
O
1
1

 

 
 
]

R

I
.
s
c
[
 
 

1
v
2
1
3
3

.

0
1
2
1
:
v
i
X
r
a

Abstract

This paper describes Artex, another algorithm for Automatic Text Summarization.
In order to rank sentences, a simple inner product is calculated between each sentence, a
document vector (text topic) and a lexical vector (vocabulary used by a sentence). Sum-
maries are then generated by assembling the highest ranked sentences. No ruled-based
linguistic post-processing is necessary in order to obtain summaries. Tests over several
datasets (coming from Document Understanding Conferences (DUC), Text Analysis Con-
ference (TAC), evaluation campaigns, etc.) in French, English and Spanish have shown
that Artex summarizer achieves interesting results.

Keywords: Automatic Text Summarization, Space Vector Model, Text extraction, Ultra-
stemming

1

Introduction

Automatic Text Summarization (ATS) is the process to automatically generate a compressed
version of a source document [15]. Query-oriented summaries focus on a user’s request, and
extract the information related to the speciﬁed topic given explicitly in the form of a query
[2]. Generic mono-document summarization tries to cover as much as possible the informa-
tion content. Multi-document summarization is a oriented task to create a summary from
a heterogeneous set of documents on a focused topic. Over the past years, extensive exper-
iments on query-oriented multi-document summarization have been carried out. Extractive
Summarization produces summaries choosing a subset of representative sentences from original
documents. Sentences are ordered and then assembled according to their relevance to generate
the ﬁnal summary [10].

This article introduces a new method of summarization based in sentences extraction on
Vector Space Model (VSM). We score each sentence by calculating their inner product with a
pseudo-sentence vector and a pseudo-word vector. Results show that Artex not only preserves
the content of the summaries generated using this new representation, but often, surprisingly
the performance can be improved. Artex could be an interesting and simple algorithm using
the extractive summarization paradigm. Our tests on trilingual corpora (English, Spanish
and French) evaluated by the Fresa algorithm (without human references) conﬁrm the good
performance of Artex.

In this paper, related work is given in Section 2. Section 3 presents the new algorithm of
Automatic Text Summarization. Experiments are presented in Section 4, followed by Results
in Section 5 and Conclusions in Section 6.

1

2 Related works

Research in Automatic Text Summarization was introduced by H.P. Luhn in 1958 [9]. In the
strategy proposed by Luhn, the sentences are scored for their component word values as de-
termined by tf*idf-like weights. Scored sentences are then ranked and selected from the top
until some summary length threshold is reached. Finally, the summary is generated by assem-
bling the selected sentences in the original source order. Although fairly simple, this extractive
methodology is still used in current approaches. Later on, [3] extended this work by adding sim-
ple heuristic features such as the position of sentences in the text or some key phrases indicate
the importance of the sentences. As the range of possible features for source characterization
widened, choosing appropriate features, feature weights and feature combinations have become
a central issue.

A natural way to tackle this problem is to consider sentence extraction as a classiﬁcation
task. To this end, several machine learning approaches that uses document-summary pairs
have been proposed [6, 12], An hybrid method mixing statistical and linguistics algorithms is
presented in [1].
[10] and [15] propose a good state-of-art of Automatic Text Summarization
tasks and algorithms.

2.1 Document Pre-processing
The ﬁrst step to represent documents in a suitable space is the pre-processing. As we use
extractive summarization, documents have to be chunked into cohesive textual segments that
will be assembled to produce the summary. Pre-processing is very important because the
selection of segments is based on words or bigrams of words. The choice was made to split
documents into full sentences, in this way obtaining textual segments that are likely to be
grammatically corrects. Afterwards, sentences pass through several basic normalization steps
in order to reduce computational complexity.

The process is composed by the following steps:

1. Sentence splitting. Simple rule-based method is used for sentence splitting. Documents

are chunked at the period, exclamation and question mark.

2. Sentence ﬁltering. Words lowercased and cleared up from sloppy punctuation. Words
with less than 2 occurrences (f < 2) are eliminated (Hapax legomenon presents once in a
document). Words that do not carry meaning such as functional or very common words
are removed. Small stop-lists (depending of language) are used in this step.

3. Word normalization. Remaining words are replaced by their canonical form using
lemmatization, stemming, ultra-stemming or none of them (raw text). Four methods of
normalization were applied after ﬁltering:

• Lemmatization by simples dictionaries of morphological families. These dictionaries
have 1.32M, 208K and 316K words-entries in Spanish, English and French, respec-
tively.

• Porter’s Stemming, available at Snowball (web site http://snowball.tartarus.
org/texts/stemmersoverview.html) for English, Spanish, French among other lan-
guages.

• Ultra-stemming. This normalization seems be very eﬃcient and it produces a com-
pact matrix representation [16]. Ultra-stemming consider only the n ﬁrst letters of

each word. For example, in the case of ultra-stemming (ﬁrst letter, Fix1), inﬂected
verbs like “sing”, “song”, “sings”, “singing”... or proper names “smith”, “snowboard”,
“sex”,... are replaced by the letter “s”.

4. Text Vectorization. Documents are vectorized in a matrix S[P×N ] of P sentences and
N columns. Each element si,j represents the occurrences of an object j (a letter in the
case of ultra-stemming, a word in the case of lemmatization or a stem for stemming),
j = 1, 2, ..., N in the sentence i, i = 1, 2, ..., P .

3 AnotheR TEXt summarizer (Artex)
Artex1 is a simple extractive algorithm for Automatic Text Summarization. The main idea is
the next one: First, we represent the text in a suitable space model (VSM). Then, we construct
an average document vector that represents the average (the “global topic”) of all sentences
vectors. At the same time, we obtain the “lexical weight” for each sentence, i.e. the number of
words in the sentence. After that, it is calculated the angle between the average document and
each sentence; narrow angles indicate that the sentences near of the “global topic” should be
important and therefore extracted. See on the ﬁgure 1 the VSM of words: P vector sentences
and the average “global topic” are represented in a N dimensional space of words.

Figure 1: The “global topic” in a Vector Space Model of N words.

Next, a score for each sentence is calculated using their proximity with the “global topic”
and their “lexical weight”. In the ﬁgure 2, the “lexical weight” is represented in a VSM of P
sentences.

Finally, the summary is generated concatenating the sentences with the highest scores fol-

lowing their order in the original document.

1In French, Artex est un Autre Résumeur TEXtuel.

  bsis1s2s3sisPw1wjwNacos a=(b • si )/||b|| ||si||VSM wordsGlobal topicSentenceAngleFigure 2: The “lexical weight” in a Vector Space Model of P sentences.

3.1 Algorithm
Formally, Artex algorithm computes the score of each sentence by calculating the inner product
between a sentence vector, an average pseudo-sentence vector (the “global topic”) and an average
pseudo-word vector (the “lexical weight”).

Once a pre-processing (word normalization and ﬁltering of stop words) is completed, it is
created a matrix S[P×N ], using the Vector Space Model, that contains N words (or letters) and
P sentences.

Let si = (s1, s2, ..., sN ) be a vector of the sentence i, i = 1, 2, ..., P . We deﬁned (cid:126)a the average
pseudo-word vector, as the average number of occurrences of N words used in the sentence i:

and (cid:126)b the average pseudo-sentence vector as the average number of occurrences of each word j
used trough the P sentences:

(1)

ai =

1
N

si,j

(2)

bj =

1
P

(cid:88)

j

(cid:88)

i

si,j

(cid:16)

The score or weight of each sentence si is calculated as follows:

(cid:17) × (cid:126)a =

1
N P

(cid:88)

j

 × ai ; i = 1, 2, ..., P ; j = 1, 1, ..., N

si,j × bj

(3)

score(si) =

(cid:126)s × (cid:126)b

The score(•) computed by equation 3 must be normalized between the interval [0,1]. The
calculation of (cid:126)s × (cid:126)b indicates the proximity between the sentence (cid:126)s and the average pseudo-
sentence (cid:126)b. The product ((cid:126)s × (cid:126)b) × (cid:126)a weigh this proximity using the average pseudo-word ai.

  awjw1w2wNs1sisPVSM sentencesLexical weightIf a sentence si is near of (cid:126)b and their corresponding element ai has a high value, si will have,
therefore, a high score. Moreover, a sentence i far of main topic (i.e. (cid:126)si × (cid:126)b is near 0) or a less
informative sentence i (i.e. ai are near 0) will have a low score.

N P , because the angle α = arccos(cid:126)b.(cid:126)s/|(cid:126)b||(cid:126)s| between (cid:126)b and (cid:126)s is the same if we use (cid:126)b = (cid:126)b(cid:48) =(cid:80)

In computational terms, it is not really necessary to divide the scalar product by the constant
i si,j.

[P×N ], where each element
2, we can normalize vectors (cid:126)a, (cid:126)b and matrix S, as follows:

In fact, if the matrix S[P×N ] is approximated to a binary matrix2 S(cid:48)

1
The element ai is only a scale factor that does not modify α.
i,j = {0, 1} has a probability of p = 1
s(cid:48)
(cid:113)
P(cid:88)
(cid:113)
N(cid:88)
(cid:113)
N(cid:88)

(cid:113)
(cid:113)
(cid:112){0, 1}2 = N

P(cid:88)
N(cid:88)
N(cid:88)

({0, 1}P )2 = N

({0, 1}N )2 =

|(cid:126)a| =

|(cid:126)b| =

|(cid:126)si| =

2 =

2 =

2 =

(6)

(4)

(5)

s(cid:48)

i,j

j

s(cid:48)

i,j

j

i

i

√

P

√

N P

s(cid:48)

i,j

j

j

Vectors then will be represented in hyper-spheres of N or P dimensions, and the normalized

score’ in this space would be:

(cid:32)

score’(si) =

(cid:126)s

|(cid:126)s| × (cid:126)b
|(cid:126)b|

× (cid:126)a

|(cid:126)a| =

(7)

=

1√
N 5P 3

si,j × bj

(cid:33)
(cid:88)

j

1

√

(cid:88)

 × ai
 × ai ; i = 1, 2, ..., P ; j = 1, 2, ..., N

si,j × bj

N P N

√

P

j

N

√
However, the term 1/
N 5P 3 is a constant value (i.e. a simple scale factor), and then the
score(•) calculated using the equation 3) and the score’(•) using the equation 7, are both
equivalent.

4 Experiments
Artex algorithm described in the previous section has been implemented and evaluated in
corpora in several languages.

We have conducted our experimentation with the following languages, summarization tasks,
summarizers and data sets: 1) Generic multi-document-summarization in English with the
corpus DUC’04; 2) Generic single-document summarization in Spanish with the corpus Medicina
Clínica and 3) Generic single document summarization in French with the corpus Pistes.

We have applied the summarization algorithms and ﬁnally, the results have been evaluated
using Fresa while processing times for each summarizer have been measured and compared.
The following subsections present formally the details of the summarizers, corpora and

evaluations studied in diﬀerent experiments.

2This is a reasonable approximation in this context, because S[P×N ] is a sparsed matrix with many term

occurrences equal to one or zero.

4.1 Other Summarizers
To compare the performances, two other summarization systems were used in our experiments:
Cortex and Enertex. To be in the same conditions, these two systems have used exactly the
same textual representation based on Vector Space Model, described in Section 2.1.

• Cortex is a single-document summarization system using several metrics and an optimal

decision algorithm [4, 14, 15, 18].

• Enertex is a summarization system based in Textual Energy concept [5]: text is repre-
sented as a spin system where spins ↑ represents words that their occurrences are f > 1
(spins ↓ if the word is not present).

4.2 Summarization Corpora Description
To study the impact of our summarizer, we used corpora in three languages: English, Spanish
and French. The corpora are heterogeneous, and diﬀerent tasks are representatives of Auto-
matic Text Summarization: generic multi-document summary and mono-document guided by
a subject.

• Corpus in English. Piloted by NIST in Document Understanding Conference3 (DUC) the
Task 2 of DUC’044, aims to produce a short summary of a cluster of related documents.
We studied generic multi-document-summarization in English using data from DUC’04.
This corpus with 300K words (17 780 types) is compound of 50 clusters, 10 documents
each.

• Corpus in Spanish. Generic single-document summarization using a corpus from the
scientiﬁc journal Medicina Clínica5, which is composed of 50 medical articles in Spanish,
each one with its corresponding author abstract. This corpus contains 125K words (9 657
types).

• Corpus in French. We have studied generic single-document summarization using the
Canadian French Sociological Articles corpus, generated from the journal Perspectives
interdisciplinaires sur le travail et la santé (Pistes)6. It contains 50 sociological articles
in French, each one with its corresponding author abstract. This corpus contains near
400K words (18 887 types).

4.3 Summaries Content Evaluation
DUC conferences have introduced the ROUGE content evaluation [7], wich measures the over-
lap of n-grams between a candidate summary and reference summaries written by humans.
However, to write the human summaries necessaries for ROUGE is a very expensive task.

Recently metrics without references have been deﬁned and experimented at DUC and Text

Analysis Conferences (TAC)7 workshops.

Fresa content evaluation [13, 17] is similar to ROUGE evaluation, but human reference
summaries are not necessary. Fresa calculates the divergence of probabilities between the

3http://duc.nist.gov
4http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html
5http://www.elsevier.es/revistas/ctl_servlet?_f=7032&revistaid=2
6http://www.pistes.uqam.ca/
7www.nist.gov/tac

candidate summary and the document source. Among these metrics, Kullback-Leibler (KL)
and Jensen-Shannon (JS) divergences have been widely used by [8, 17] to evaluate the informa-
tiveness of summaries.

In this article, we use Fresa, based in KL divergence with Dirichlet smoothing, like in
the 2010 and 2011 INEX edition [11], to evaluate the informative content of summaries by
comparing their n-gram distributions with those from source documents.

Fresa only considered absolute log-diﬀ between the terms occurrences of the source and
its

the summary. Let T be the set of terms in the source. For every t ∈ T , we denote by C T
occurrences in the source and C S
t

its occurrences in the summary.

t

The Fresa package computed the divergence between the document source and the sum-

maries as follows:

(8) D(T||S) =

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:88)

t∈T

(cid:19)

(cid:18) C T

t|T| + 1

(cid:18) C S

t|S| + 1

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)

− log

To evaluate the information content (the “quality”) of the generated summaries, after re-
moving stop-words, several automatic measures were computed: Fresa1 (Unigrams of single
stems), Fresa2 (Bigrams of pairs of consecutive stems), FresaSU 4 (Bigrams with 2-gaps also
made of pairs of consecutive stems) and ﬁnally, (cid:104)Fresa(cid:105), i.e. the average of all Fresa values.
The Fresa values (scores) are normalized between 0 and 1. High Fresa values mean less
divergence regarding the source document summary, reﬂecting a greater amount of information
content. All summaries produced by the systems were evaluated automatically using Fresa
package.

5 Results

In this section we present the results for each corpus with diﬀerent summarizers and the sev-
eral normalization strategies used. Based on these results, ﬁrstly, we have veriﬁed that ultra-
stemming improves the performance of summarizers. Secondly, we show that Artex is a system
that has a similar performances –in terms of information content and processing times– to other
state-of-art summarizers.

5.1 Content evaluation

• English corpus. Figure 3 shows the performance of the three summarizers using Fix1,
stemming and lemmatization. Results show that ultra-stemming improves the score of the
three automatic summarizer systems. Artex and Cortex expose a similar performances
in information content.

Figure 3: Histogram plot of content evaluation for corpus DUC’04 Task 2, with (cid:104)Fresa(cid:105) mea-
sures, for each summarizer and each normalization.

• Spanish corpus. Spanish is a language with a greater variability than English. Results in
ﬁgure 4 shown that Artex summarizer outperforms Cortex and Enertex if stemming
or lemmatization are used as normalization.

Figure 4: Histogram plot of content evaluation for Spanish corpus Medicina Clínica with
(cid:104)Fresa(cid:105) scores for each summarizer.

(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:6)(cid:4)(cid:2)(cid:3)(cid:4)(cid:5)(cid:7)(cid:8)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)(cid:10)(cid:9)(cid:9)(cid:9)(cid:9)(cid:10)(cid:9)(cid:9)(cid:11)(cid:9)(cid:10)(cid:9)(cid:12)(cid:9)(cid:9)(cid:10)(cid:9)(cid:12)(cid:11)(cid:9)(cid:10)(cid:9)(cid:13)(cid:9)(cid:0)(cid:0)(cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:4)(cid:0)(cid:9)(cid:10)(cid:11)(cid:0)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:7)(cid:20)(cid:7)(cid:21)(cid:10)(cid:7)(cid:0)(cid:16)(cid:22)(cid:18)(cid:23)(cid:24)(cid:25)(cid:18)(cid:7)(cid:26)(cid:27)(cid:18)(cid:28)(cid:7)(cid:29)(cid:30)(cid:7)(cid:31)(cid:24)(cid:14)(cid:24)(cid:25)(cid:17) (cid:7)(cid:18)(cid:22)!!(cid:27)(cid:25)(cid:17)"(cid:27)(cid:23)(cid:17)#(cid:14)$(cid:14)(cid:15)(cid:2)(cid:4)(cid:16)(cid:1)(cid:17)(cid:16)(cid:18)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:23)(cid:24)(cid:25)(cid:12)(cid:0)(cid:26)(cid:21)(cid:22)(cid:22)(cid:0)(cid:19)(cid:20)(cid:21)(cid:22)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:6)(cid:4)(cid:2)(cid:3)(cid:4)(cid:5)(cid:7)(cid:8)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)(cid:10)(cid:9)(cid:9)(cid:9)(cid:10)(cid:9)(cid:11)(cid:9)(cid:10)(cid:9)(cid:12)(cid:9)(cid:10)(cid:9)(cid:13)(cid:9)(cid:10)(cid:9)(cid:14)(cid:9)(cid:10)(cid:15)(cid:9)(cid:9)(cid:10)(cid:15)(cid:11)(cid:9)(cid:10)(cid:15)(cid:12)(cid:9)(cid:10)(cid:15)(cid:13)(cid:9)(cid:10)(cid:15)(cid:14)(cid:0)(cid:0)(cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:0)(cid:11)(cid:12)(cid:13)(cid:7)(cid:0)(cid:14)(cid:11)(cid:12)(cid:11)(cid:0)(cid:13)(cid:15)(cid:16)(cid:3)(cid:17)(cid:18)(cid:19)(cid:5)(cid:20)(cid:7)(cid:21)(cid:7)(cid:22)(cid:23)(cid:7)(cid:24)(cid:1)(cid:25)(cid:5)(cid:7)(cid:21)(cid:7)(cid:26)(cid:4)(cid:19)(cid:24)(cid:27)(cid:24)(cid:7)(cid:5)(cid:4)(cid:28)(cid:28)(cid:17)(cid:2)(cid:19)(cid:29)(cid:17)(cid:30)(cid:19)(cid:1)(cid:18)(cid:31)(cid:16)(cid:17)(cid:2)(cid:4)(cid:18)(cid:1)(cid:19)(cid:18)(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:0)(cid:25)(cid:26)(cid:27)(cid:15)(cid:0)(cid:28)(cid:23)(cid:24)(cid:24)(cid:0)(cid:21)(cid:22)(cid:23)(cid:24)• French corpus. French is a language with a large variability too. Figure 5 shows the
score (cid:104)Fresa(cid:105) on the French corpus Pistes. Results show a similar behavior: Ultra-
stemming improves the score of the three automatic summarization systems used.
In
particular, the eﬃcacy of Artex is less sensible to word normalization than others sum-
marizers.

Figure 5: Histogram plot of content evaluation for French corpus Pistes with (cid:104)Fresa(cid:105) scores
for each summarizer.

5.2 Processing Times Evaluation
Table 1 shows processing times for each corpus, following the normalization method for Cortex,
Artex and Enertex summarizers8. Processing times of ultra-stemming Fix1 are shorter
compared to all others methods. By example, Cortex is a very fast summarizer with O(log ρ2)
(where ρ = P × N), and processing times for stemming and Fix1 are close. In other hand,
Enertex summarizer has a complexity of O(ρ2), then it needs more time to process the same
corpus. Performances of Artex algorithm remain close to Cortex.

Summarizer Average Time

(all corpora)

Normalization Cortex Artex Enertex
10.42’
Lemmatization
9.47’
Stemming
fix1
4.25’

1.60’
0.54’
0.32’

2.50’
1.29’
0.40’

Table 1: Statistics of processing times (in minutes) of three summarizers over three corpora.

8All times are measured in a 7.8 GB of RAM computer, Core i7-2640M CPU @ 2.80GHz × 4 processor,

running under 32 bits GNU/Linux (Ubuntu Version 12.04).

(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:0)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:6)(cid:4)(cid:2)(cid:3)(cid:4)(cid:5)(cid:7)(cid:8)(cid:2)(cid:3)(cid:4)(cid:5)(cid:9)(cid:10)(cid:9)(cid:9)(cid:9)(cid:10)(cid:9)(cid:11)(cid:9)(cid:10)(cid:12)(cid:9)(cid:9)(cid:10)(cid:12)(cid:11)(cid:9)(cid:10)(cid:13)(cid:9)(cid:9)(cid:10)(cid:13)(cid:11)(cid:0)(cid:0)(cid:7)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:0)(cid:19)(cid:20)(cid:21)(cid:3)(cid:4)(cid:21)(cid:0)(cid:22)(cid:23)(cid:15)(cid:24)(cid:25)(cid:26)(cid:27)(cid:0)(cid:28)(cid:0)(cid:11)(cid:9)(cid:0)(cid:29)(cid:14)(cid:26)(cid:18)(cid:0)(cid:28)(cid:0)(cid:30)(cid:24)(cid:25)(cid:24)(cid:15)(cid:31)(cid:26)(cid:0)(cid:18)(cid:17)  !(cid:15)(cid:31)"!#(cid:31)(cid:14)(cid:25)$%(cid:23)(cid:2)(cid:4)(cid:21)(cid:1)&(cid:21)(cid:17)  !(cid:15)(cid:31)"(cid:24)(cid:15)(cid:0)’(cid:31)((cid:12)(cid:0))(cid:24)  (cid:0)(cid:18)#(cid:24) 6 Conclusions

In this article we have introduced and tested a simple method for Automatic Text Summa-
rization. Artex is a fast and very simple algorithm based in VSM model and the extractive
paradigm. The method uses a matrix representation to calculate a normalized score for each
sentence, using the inner product of pseudo-(sentences|words) vectors. The algorithm retains
the salient information of each sentence of document. An important aspect of our approach is
that it does not requires linguistic knowledge or resources which makes it a simple and eﬃcient
summarizer method to tackle the issue of Automatic Text Summarization.

Summaries generated by Artex system are pertinents. The results obtained on corpora
in English, Spanish and French show that Artex can achieve good results for content quality.
Tests with other corpora (DUC and TAC evaluation campaigns, INEX, etc.)
in mono-and
multi-document guided by a subject, using content evaluation with (ROUGE evaluations) or
without reference summaries still in progress.

References

[1] Iria da Cunha, Silvia Fernández, Patricia Velázquez-Morales, Jorge Vivaldi, Eric SanJuan, and
Juan Manuel Torres-Moreno. A new hybrid summarizer based on vector space model, statistical
physics and linguistics. In Proceedings of the 6th Mexican International Conference on Advances in
Artiﬁcial Intelligence (MICAI’07), pages 872–882, Aguascalientes, Mexico, 2007. Springer-Verlag.
[2] Harold Daumé III. Practical structured learning techniques for natural language processing. PhD

thesis, Los Angeles, CA, 2006.

[3] H. P. Edmundson. New Methods in Automatic Extraction. Journal of the Association for Com-

puting Machinery, 16(2):264–285, 1969.

[4] B. Favre, F. Béchet, P. Bellot, F. Boudin, M. El-Bèze, L. Gillard, G. Lapalme, and J-M. Torres-
Moreno. The LIA-Thales summarization system at DUC-2006. In Proceedings of the Document Un-
derstanding Conference (DUC’06), Brooklyn, New York, United States, 2006. http://duc.nist.gov.
[5] Silvia Fernández, Eric SanJuan, and Juan-Manuel Torres-Moreno. Textual Energy of Associa-
tive Memories: performants applications of Enertex algorithm in text summarization and topic
segmentation. In Proceedings of the Mexican International Conference on Artiﬁcial Intelligence
(MICAI’07), pages 861–871, Aguascalientes, Mexico, 2007. Springer-Verlag.

[6] J. Kupiec, J. Pedersen, and F. Chen. A trainable document summarizer. In Proceedings of the
18th Conference ACM Special Interest Group on Information Retrieval (SIGIR’95), pages 68–73,
Seattle, WA, United States, 1995. ACM Press, New York.

[7] Chin-Yew Lin. ROUGE: A Package for Automatic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Proceedings of the Workshop Text Summarization Branches
Out (ACL’04), pages 74–81, Barcelone, Spain, july 2004. ACL.

[8] Annie Louis and Ani Nenkova. Automatic Summary Evaluation without Human Models. In First

Text Analysis Conference (TAC’08), Gaithersburg, MD, United States, 17-19 November 2008.

[9] H.P. Luhn. The Automatic Creation of Literature Abstracts.

Development, 2(2):159–165, 1958.

IBM Journal of Research and

[10] I. Mani and M. Mayburi. Advances in Automatic Text Summarization. MIT Press, Cambridge,

1999.

[11] Eric SanJuan, Patrice Bellot, Véronique Moriceau, and Xavier Tannier. Overview of the INEX
2010 Question Answering Track (QA@INEX). In Shlomo Geva, Jaap Kamps, Ralf Schenkel, and
Andrew Trotman, editors, Comparative Evaluation of Focused Retrieval, volume 6932 of Lecture
Notes in Computer Science, pages 269–281. Springer Berlin / Heidelberg, 2011.

[12] Simone Teufel and Marc Moens. Sentence extraction as a classiﬁcation task.

In I. Mani and
M. Maybury, editors, Proceedings of the ACL/EACL’97 Workshop on Intelligent Scalable Text
Summarization, Madrid, Spain, 11 July 1997.

[13] J.-M. Torres-Moreno, Horacio Saggion, I. da Cunha, P. Velazquez-Morales, and E. SanJuan. Eval-
uation automatique de résumés avec et sans réferences. In Proceedings de la conference Traite-
ment Automatique des Langagues Naturelles (TALN’10), Montréal, QC, Canada, 19-23 July 2010.
ATALA.

[14] J.-M. Torres-Moreno, P.-L. St-Onge, M. Gagnon, M. El-Bèze, and P. Bellot. Automatic Sum-
marization System coupled with a Question-Answering System (QAAS). CoRR, abs/0905.2990,
2009.

[15] Juan-Manuel Torres-Moreno. Résumé automatique de documents: une approche statistique.

Hermès-Lavoisier, Paris, 2011.

[16] Juan-Manuel Torres-Moreno. Beyond Stemming and Lemmatization: Ultra-stemming to Improve

Automatic Text Summarization. CoRR, arXiv:1209.3126 [cs.IR], 2012.

[17] Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, and Eric SanJuan. Summary Evalu-
ation With and Without References. Polibits: Research journal on Computer science and computer
engineering with applications, 42:13–19, 2010.

[18] Juan-Manuel Torres-Moreno, Patricia Velázquez-Morales, and Jean-Guy Meunier. Cortex : un
In Proceedings of the Conference de

algorithme pour la condensation automatique des textes.
l’Association pour la Recherche Cognitive, volume 2, pages 365–366, Lyon, France, 2001.

